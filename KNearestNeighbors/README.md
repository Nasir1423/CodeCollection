# K 近邻

## 1. 最近邻

<img src="https://raw.githubusercontent.com/Nasir1423/blog-img/main/image-20231120133324711.png" alt="image-20231120133324711" style="zoom:50%;" />

> 当样本数目有限时，**样本分布可能带有很大的偶然性**，不一定能很好地代表数据内在的分布情况，此时就会影响最近邻法的性能。当数据**内在规律较复杂、类别间存在交叠**等情况下尤其如此。

## 2. K 近邻

<img src="https://raw.githubusercontent.com/Nasir1423/blog-img/main/image-20231120134011931.png" alt="image-20231120134011931" style="zoom:50%;" />

> 近邻法只是确定一种决策规则，并不需要已知数据事先训练出一个判别函数，而是**在面对新样本时直接根据已知样本进行决策**。这种决策方法**计算和存储成本都很大**，因为需要将每一个新样本和所有已知样本进行比较和排序，且存储所有已知样本。

> 此外，在利用 K 近邻判别样本类别的时候，需要对样本数据进行**标准化处理**，避免量纲不同导致距离计算的特征偏好差异。

### 2.1 K 值选择

对于二分类问题，**通常选择 k 为奇数**，以避免出现两类得票相等的情况。

对于多分类问题，如果出现两类得票相等的情况，则需要引进其他的机制来进行决策。

### 2.2 稀疏样本加权处理

<img src="https://raw.githubusercontent.com/Nasir1423/blog-img/main/image-20231120134257128.png" alt="image-20231120134257128" style="zoom:50%;" />

